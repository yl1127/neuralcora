{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pg6JF9P3hZpk"
   },
   "source": [
    "# Quickstart guide\n",
    "\n",
    "In this notebook we will through all the steps from downloading the data and training a model to evaluating the results. Check out the `environment.yml` file for the required Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVEOOlnrhZpl"
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "false",
    "id": "kfNdhoMehZpl"
   },
   "source": [
    "## Downloading the data\n",
    "\n",
    "The data is hosted here. For this guide we will simply download the a year (1997) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIKzFhjnhZpl"
   },
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "false",
    "id": "BVAebOELhZpm"
   },
   "source": [
    "## Open the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QCMuyjpchxp-",
    "outputId": "9688e36e-9e17-4c26-f8e6-5e600c071138"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aFpZH0qrhZpm",
    "outputId": "dfeed936-6917-4792-ee0e-50d2ed7d9f3d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = \"/Volumes/T5/neuralcora-data\"\n",
    "\n",
    "# List all files in the directory\n",
    "datasets = [f for f in os.listdir(data_dir) if f.endswith(\".nc\")]\n",
    "datasets.sort()  # sort for readability\n",
    "\n",
    "print(\"Available datasets:\")\n",
    "for ds in datasets:\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKXAv5a7hZpm"
   },
   "outputs": [],
   "source": [
    "mask = xr.open_dataset(f\"{data_dir}/real_land_mask_180_360.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0f5rvQ4hZpn"
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "def _files_for_years(start_year: int, end_year: int):\n",
    "    \"\"\"Return existing file paths for years in [start_year, end_year].\"\"\"\n",
    "    files = []\n",
    "    years = []\n",
    "    for y in range(start_year, end_year + 1):\n",
    "        fn = os.path.join(data_dir, f\"NY_{y}_180_360.nc\")\n",
    "        if os.path.exists(fn):\n",
    "            files.append(fn)\n",
    "            years.append(y)\n",
    "        else:\n",
    "            # silently skip missing years (e.g., 1997) but keep going\n",
    "            pass\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No yearly files found between {start_year} and {end_year}.\")\n",
    "    return files, years\n",
    "\n",
    "def _safe_end_of_day(ts: pd.Timestamp) -> pd.Timestamp:\n",
    "    \"\"\"If user gave a date without time, extend to end-of-day (inclusive).\"\"\"\n",
    "    if ts.time() == pd.Timestamp(0).time():  # 00:00:00\n",
    "        # move to 23:59:59.999999999 for inclusive slice\n",
    "        return ts + pd.Timedelta(days=1) - pd.Timedelta(nanoseconds=1)\n",
    "    return ts\n",
    "\n",
    "def parse_period(spec: str):\n",
    "    \"\"\"\n",
    "    Accepts:\n",
    "      - 'YYYY-YYYY'                (year range)\n",
    "      - 'YYYYMMDD-YYYYMMDD'        (date range)\n",
    "      - 'YYYYMMDDHH-YYYYMMDDHH'    (hourly datetimes)\n",
    "    Returns (start_ts, end_ts).\n",
    "    \"\"\"\n",
    "    spec = spec.strip()\n",
    "    if \"-\" not in spec:\n",
    "        raise ValueError(\"Period must be a range like '1998-2022' or '19980101-20101231'.\")\n",
    "\n",
    "    left, right = spec.split(\"-\", 1)\n",
    "    # Year-only range\n",
    "    if len(left) == 4 and len(right) == 4:\n",
    "        start = pd.to_datetime(f\"{left}-01-01 00:00:00\")\n",
    "        # end of last year: inclusive\n",
    "        end = pd.to_datetime(f\"{right}-12-31 23:59:59\")\n",
    "        return start, end\n",
    "\n",
    "    # Flexible datetime parsing for day or hour resolution\n",
    "    # e.g., 19970101, 20101231, 1997010100, 2010123123\n",
    "    def _parse_side(s):\n",
    "        if len(s) == 8:    # YYYYMMDD\n",
    "            return pd.to_datetime(s, format=\"%Y%m%d\")\n",
    "        elif len(s) == 10: # YYYYMMDDHH\n",
    "            return pd.to_datetime(s, format=\"%Y%m%d%H\")\n",
    "        else:\n",
    "            # Let pandas try, but it's better to be explicit above.\n",
    "            return pd.to_datetime(s)\n",
    "\n",
    "    start = _parse_side(left)\n",
    "    end   = _parse_side(right)\n",
    "    # Make end inclusive if no explicit time provided\n",
    "    end = _safe_end_of_day(end)\n",
    "    return start, end\n",
    "\n",
    "def open_period(period_spec: str, chunks=None, parallel=False):\n",
    "    \"\"\"\n",
    "    Open hourly data for the requested period, concatenated along time,\n",
    "    then return the dataset sliced to the exact time window.\n",
    "    - period_spec examples: '1998-2022', '19970101-20101231', '1998010100-1998123123'\n",
    "    - chunks: dict for dask chunking, e.g. {'time': 24*30} (optional; requires dask)\n",
    "    \"\"\"\n",
    "    start_ts, end_ts = parse_period(period_spec)\n",
    "    start_year, end_year = start_ts.year, end_ts.year\n",
    "\n",
    "    files, _ = _files_for_years(start_year, end_year)\n",
    "\n",
    "    # Efficient multi-file open: rely on 'time' coords to align\n",
    "    # If you have dask installed, pass e.g. chunks={'time': 24*30}\n",
    "    ds = xr.open_mfdataset(\n",
    "        files,\n",
    "        combine=\"by_coords\",\n",
    "        parallel=parallel,\n",
    "        chunks=chunks,\n",
    "        engine=\"netcdf4\"\n",
    "    )\n",
    "\n",
    "    # Clip to requested time range (inclusive)\n",
    "    ds = ds.sel(time=slice(start_ts, end_ts))\n",
    "    if ds.sizes.get(\"time\", 0) == 0:\n",
    "        raise ValueError(f\"No data within requested window {start_ts} to {end_ts}.\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gmc401CjhZpn"
   },
   "outputs": [],
   "source": [
    "# --- Examples ---\n",
    "\n",
    "# (A) Full-year span (matches what you asked):\n",
    "# Note: your data starts at 1998; 1997 will be skipped automatically.\n",
    "TIME_PERIOD = \"2020-2022\"\n",
    "\n",
    "ds = open_period(TIME_PERIOD)\n",
    "\n",
    "# (B) Date span:\n",
    "# ds = open_period(\"19970101-20101231\")\n",
    "\n",
    "# (C) Hourly precision:\n",
    "# ds = open_period(\"1998010100-1998123123\")\n",
    "\n",
    "# (D) With dask chunking (if dask installed) for better performance:\n",
    "# ds = open_period(\"1998-2022\", chunks={'time': 24*30}, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fGv6ek1YhZpn",
    "outputId": "5bd0f2a3-683b-4a64-9f03-b819a03c411b"
   },
   "outputs": [],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_LLIzAZhZpn",
    "outputId": "8d7f9c10-0702-4a28-b29e-ebd808e3651c"
   },
   "outputs": [],
   "source": [
    "# Select: zeta\n",
    "zeta = (ds[\"zeta\"].sel(time=slice(\"2020-01-01T00:00:00\", \"2022-12-31T23:00:00\")))\n",
    "\n",
    "# Quick sanity checks\n",
    "print(zeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "sfBdj8pbhZpn",
    "outputId": "b48b2e0c-4604-4262-dc71-1f70f7389a53"
   },
   "outputs": [],
   "source": [
    "zeta_val = (ds[\"zeta\"].sel(time=slice(\"2022-01-01T00:00:00\", \"2022-12-31T23:00:00\")))\n",
    "zeta_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "h0TD_bFYhZpo",
    "outputId": "9204fd6c-ce17-4500-9a8b-0a3a8611c84c"
   },
   "outputs": [],
   "source": [
    "str(zeta.time[0].values)[:19]\n",
    "# '1998-01-01T00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "TjFu0NLrhZpo",
    "outputId": "10ebd091-03cc-46ae-9e73-2d218ec6d8a6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "qm = zeta.isel(time=0).plot(ax=ax)\n",
    "ax.set_aspect(1.5)\n",
    "ax.set_title(f\"zeta of {str(zeta.time[0].values)[:19]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "false",
    "id": "AMnm2Q0zhZpq"
   },
   "source": [
    "## Train a neural network\n",
    "\n",
    "Now let's train a simple convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GbHIXGeZhZpq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.__version__\n",
    "\n",
    "# Set reproducibility seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Priority order: CUDA > TPU > MPS > CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "elif hasattr(torch, 'xla') or 'xla' in torch.__dict__:\n",
    "    try:\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        device = xm.xla_device()\n",
    "    except ImportError:\n",
    "        device = torch.device(\"cpu\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OT7CQ6rRhZpq"
   },
   "source": [
    "### PyTorch dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23gEzLLxhZpq"
   },
   "outputs": [],
   "source": [
    "# Configuration aligned with the quickstart dataset\n",
    "DATA_DIR = Path(data_dir)\n",
    "TARGET_VAR = \"zeta\"  # variable to forecast; e.g. 'geopotential'\n",
    "# TIME_PERIOD = \"1998-2000\"  # inclusive range of years to load\n",
    "\n",
    "max_lead = 1  # hours ahead to predict\n",
    "TIME_STRIDE = max_lead  # keep every 6th hour to keep the demo lightweight\n",
    "INPUT_STEPS = 4  # number of consecutive timesteps provided to the model\n",
    "\n",
    "\n",
    "# Optional mask; set MASK_PATH to None to skip\n",
    "MASK_PATH: Optional[Path] = DATA_DIR / \"real_land_mask_180_360.nc\"\n",
    "# MASK_PATH = f\"{data_dir}/real_land_mask_180_360.nc\"\n",
    "MASK_VARIABLE = \"mask\"  # variable name inside the mask file\n",
    "MASK_THRESHOLD = 0.5  # when MASK_VALID_EQUALS is None, keep values >= threshold\n",
    "MASK_VALID_EQUALS: Optional[float] = 0.0  # e.g. set to 0.0 if 0 marks valid land\n",
    "MASK_EQUAL_TOL = 1e-6\n",
    "MASK_INVERT = False  # flip valid/invalid if your mask marks land with low values\n",
    "\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"DATA_DIR {DATA_DIR} does not exist. Adjust the path before continuing.\")\n",
    "\n",
    "\n",
    "def _files_for_years(start_year: int, end_year: int):\n",
    "    files = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        candidate = DATA_DIR / f\"NY_{year}_180_360.nc\"\n",
    "        if candidate.exists():\n",
    "            files.append(candidate)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No yearly files found between {start_year} and {end_year} in {DATA_DIR}.\")\n",
    "    return files\n",
    "\n",
    "\n",
    "def _safe_end_of_day(ts: pd.Timestamp) -> pd.Timestamp:\n",
    "    if ts.time() == pd.Timestamp(0).time():\n",
    "        return ts + pd.Timedelta(days=1) - pd.Timedelta(nanoseconds=1)\n",
    "    return ts\n",
    "\n",
    "\n",
    "def parse_period(spec: str):\n",
    "    spec = spec.strip()\n",
    "    if \"-\" not in spec:\n",
    "        raise ValueError(\"Period must look like '1998-2002' or '19980101-19991231'.\")\n",
    "    left, right = spec.split(\"-\", 1)\n",
    "    if len(left) == 4 and len(right) == 4:\n",
    "        start = pd.to_datetime(f\"{left}-01-01 00:00:00\")\n",
    "        end = pd.to_datetime(f\"{right}-12-31 23:59:59\")\n",
    "        return start, end\n",
    "\n",
    "    def _parse_side(side: str) -> pd.Timestamp:\n",
    "        if len(side) == 8:\n",
    "            return pd.to_datetime(side, format=\"%Y%m%d\")\n",
    "        if len(side) == 10:\n",
    "            return pd.to_datetime(side, format=\"%Y%m%d%H\")\n",
    "        return pd.to_datetime(side)\n",
    "\n",
    "    start = _parse_side(left)\n",
    "    end = _parse_side(right)\n",
    "    end = _safe_end_of_day(end)\n",
    "    return start, end\n",
    "\n",
    "\n",
    "def open_period(period_spec: str, chunks=None, parallel=False) -> xr.Dataset:\n",
    "    start_ts, end_ts = parse_period(period_spec)\n",
    "    files = _files_for_years(start_ts.year, end_ts.year)\n",
    "    ds = xr.open_mfdataset(\n",
    "        [str(f) for f in files],\n",
    "        combine=\"by_coords\",\n",
    "        parallel=parallel,\n",
    "        chunks=chunks,\n",
    "    )\n",
    "    ds = ds.sel(time=slice(start_ts, end_ts))\n",
    "    if ds.sizes.get(\"time\", 0) == 0:\n",
    "        raise ValueError(f\"No data within requested window {start_ts} to {end_ts}.\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def ensure_lat_lon_dims(ds: xr.Dataset) -> xr.Dataset:\n",
    "    rename = {}\n",
    "    if \"latitude\" in ds.dims and \"lat\" not in ds.dims:\n",
    "        rename[\"latitude\"] = \"lat\"\n",
    "    if \"longitude\" in ds.dims and \"lon\" not in ds.dims:\n",
    "        rename[\"longitude\"] = \"lon\"\n",
    "    return ds.rename(rename) if rename else ds\n",
    "\n",
    "\n",
    "def subset_by_years(ds: xr.Dataset, years: np.ndarray) -> xr.Dataset:\n",
    "    years = np.asarray(years, dtype=int)\n",
    "    mask = ds[\"time\"].dt.year.isin(years)\n",
    "    return ds.sel(time=mask)\n",
    "\n",
    "\n",
    "def rename_for_score(da: xr.DataArray) -> xr.DataArray:\n",
    "    rename = {}\n",
    "    if \"lat\" in da.dims and \"latitude\" not in da.dims:\n",
    "        rename[\"lat\"] = \"latitude\"\n",
    "    if \"lon\" in da.dims and \"longitude\" not in da.dims:\n",
    "        rename[\"lon\"] = \"longitude\"\n",
    "    return da.rename(rename) if rename else da\n",
    "\n",
    "\n",
    "def load_land_mask() -> Optional[xr.DataArray]:\n",
    "    if MASK_PATH is None or not MASK_PATH.exists():\n",
    "        return None\n",
    "    mask_ds = xr.open_dataset(MASK_PATH)\n",
    "    if MASK_VARIABLE not in mask_ds:\n",
    "        raise KeyError(f\"{MASK_VARIABLE!r} not found in {MASK_PATH}\")\n",
    "    mask = ensure_lat_lon_dims(mask_ds[MASK_VARIABLE])\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QV87m5RZhZpq",
    "outputId": "c98d38c5-01c7-4415-c283-01b97bb4bc74"
   },
   "outputs": [],
   "source": [
    "# Derive simple train/validation/test splits from the available years.\n",
    "available_years = np.unique(ds.time.dt.year.values)\n",
    "if available_years.size < 3:\n",
    "    raise ValueError(\n",
    "        \"TIME_PERIOD must span at least three distinct years to create train/valid/test splits.\"\n",
    "    )\n",
    "\n",
    "train_years = available_years[:-2]\n",
    "valid_years = available_years[-2:-1]\n",
    "test_years = available_years[-1:]\n",
    "\n",
    "if train_years.size == 0:\n",
    "    raise ValueError(\"Extend TIME_PERIOD so that at least one year is available for training.\")\n",
    "\n",
    "# select data subsets\n",
    "\n",
    "ds_train = subset_by_years(ds, train_years)\n",
    "ds_valid = subset_by_years(ds, valid_years)\n",
    "ds_test = subset_by_years(ds, test_years)\n",
    "\n",
    "print(f\"Train years: {train_years.tolist()}\")\n",
    "print(f\"Valid years: {valid_years.tolist()}\")\n",
    "print(f\"Test years:  {test_years.tolist()}\")\n",
    "print(ds_train[TARGET_VAR])\n",
    "print(ds_valid[TARGET_VAR])\n",
    "print(ds_test[TARGET_VAR])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "false",
    "id": "pbY0utLChZpq"
   },
   "source": [
    "First, we need to create the data generators for training, validation and testing. The main reason why we are using data generators instead of just loading the data as Numpy arrays is that this would require loading the same data twice since the features and targets are the same fields, just offset in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-bYwgX_hZpq",
    "outputId": "c50d10be-aa0c-46a5-c44e-64fbde22c77f"
   },
   "outputs": [],
   "source": [
    "# Open the requested period, keep only the target variable, and optionally thin the time dimension.\n",
    "ds_raw = open_period(TIME_PERIOD)\n",
    "ds_raw = ensure_lat_lon_dims(ds_raw)\n",
    "\n",
    "if TARGET_VAR not in ds_raw:\n",
    "    raise KeyError(f\"{TARGET_VAR!r} not found in dataset variables: {list(ds_raw.data_vars)}\")\n",
    "\n",
    "ds_raw = ds_raw[[TARGET_VAR]]\n",
    "\n",
    "land_mask = load_land_mask()\n",
    "if land_mask is not None:\n",
    "    land_mask = land_mask.interp_like(ds_raw[TARGET_VAR], method=\"nearest\")\n",
    "    if MASK_VALID_EQUALS is not None:\n",
    "        valid = np.isclose(land_mask, MASK_VALID_EQUALS, atol=MASK_EQUAL_TOL)\n",
    "    else:\n",
    "        valid = land_mask >= MASK_THRESHOLD\n",
    "    if MASK_INVERT:\n",
    "        valid = ~valid\n",
    "    ds_raw[TARGET_VAR] = ds_raw[TARGET_VAR].where(valid)\n",
    "\n",
    "if TIME_STRIDE and TIME_STRIDE > 1:\n",
    "    ds_raw = ds_raw.isel(time=slice(0, None, TIME_STRIDE))\n",
    "\n",
    "ds_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4Lo7SQbhZpq",
    "outputId": "016fc70f-20d8-430f-a6fc-6f40cc947ac9"
   },
   "outputs": [],
   "source": [
    "# Derive simple train/validation/test splits from the available years.\n",
    "available_years = np.unique(ds_raw.time.dt.year.values)\n",
    "if available_years.size < 3:\n",
    "    raise ValueError(\n",
    "        \"TIME_PERIOD must span at least three distinct years to create train/valid/test splits.\"\n",
    "    )\n",
    "\n",
    "train_years = available_years[:-2]\n",
    "valid_years = available_years[-2:-1]\n",
    "test_years = available_years[-1:]\n",
    "\n",
    "if train_years.size == 0:\n",
    "    raise ValueError(\"Extend TIME_PERIOD so that at least one year is available for training.\")\n",
    "\n",
    "# select data subsets\n",
    "\n",
    "ds_train = subset_by_years(ds_raw, train_years)\n",
    "ds_valid = subset_by_years(ds_raw, valid_years)\n",
    "ds_test = subset_by_years(ds_raw, test_years)\n",
    "\n",
    "print(f\"Train years: {train_years.tolist()}\")\n",
    "print(f\"Valid years: {valid_years.tolist()}\")\n",
    "print(f\"Test years:  {test_years.tolist()}\")\n",
    "print(ds_train[TARGET_VAR])\n",
    "print(ds_valid[TARGET_VAR])\n",
    "print(ds_test[TARGET_VAR])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MTuh0ffhZpr",
    "outputId": "629bb4f4-2030-426e-b1f6-a27571010d4d"
   },
   "outputs": [],
   "source": [
    "print(f\"Train samples: {ds_train.sizes['time']}\")\n",
    "print(f\"Valid samples: {ds_valid.sizes['time']}\")\n",
    "print(f\"Test samples: {ds_test.sizes['time']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgqNtTPRhZpr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from collections import OrderedDict\n",
    "from typing import Optional\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "def _get_lat_lon_names(ds):\n",
    "    for lat_name in (\"lat\", \"latitude\", \"y\"):\n",
    "        if lat_name in ds.dims:\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(\"Latitude dim not found. Tried: lat, latitude, y\")\n",
    "    for lon_name in (\"lon\", \"longitude\", \"x\"):\n",
    "        if lon_name in ds.dims:\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(\"Longitude dim not found. Tried: lon, longitude, x\")\n",
    "    return lat_name, lon_name\n",
    "\n",
    "def _stack_vars_levels(ds, var_dict, lat_name, lon_name):\n",
    "    \"\"\"Return DataArray of shape (time, lat, lon, level_concat) and bookkeeping dicts.\"\"\"\n",
    "    data_arrays = []\n",
    "    level_slices = OrderedDict()\n",
    "    level_coords = {}\n",
    "    offset = 0\n",
    "    for var, levels in OrderedDict(var_dict).items():\n",
    "        da = ds[var]\n",
    "        if \"level\" in da.dims:  # 3D var\n",
    "            da_sel = da if levels is None else da.sel(level=levels)\n",
    "        else:  # 2D var → add a mock level dim\n",
    "            if levels is not None:\n",
    "                raise ValueError(f\"{var} does not have 'level' dim\")\n",
    "            da_sel = da.expand_dims(level=[1])\n",
    "        da_sel = da_sel.transpose(\"time\", lat_name, lon_name, \"level\")\n",
    "        data_arrays.append(da_sel)\n",
    "\n",
    "        nlev = da_sel.sizes[\"level\"]\n",
    "        level_slices[var] = slice(offset, offset + nlev)\n",
    "        level_coords[var] = None if levels is None else np.array(da_sel.level.values)\n",
    "        offset += nlev\n",
    "\n",
    "    data = xr.concat(data_arrays, dim=\"level\")  # (time, lat, lon, level_concat)\n",
    "    return data, level_slices, level_coords\n",
    "\n",
    "def _compute_stats(data, lat_name, lon_name, mask=None, weights=None):\n",
    "    \"\"\"Compute mean/std per channel (level) over (time, lat, lon).\"\"\"\n",
    "    if mask is not None:\n",
    "        data = data.where(mask == 0)\n",
    "\n",
    "    reduce_dims = (\"time\", lat_name, lon_name)\n",
    "\n",
    "    if weights is not None:\n",
    "        if weights.dims == (lat_name,):\n",
    "            weights = weights.broadcast_like(data.isel(time=0, level=0))\n",
    "        if mask is not None:\n",
    "            weights = weights.where(mask == 0, 0)\n",
    "        wmean = data.weighted(weights).mean(reduce_dims, skipna=True)\n",
    "        wmean_sq = (data ** 2).weighted(weights).mean(reduce_dims, skipna=True)\n",
    "        wvar = (wmean_sq - wmean ** 2).clip(min=0.0)\n",
    "        wstd = np.sqrt(wvar)\n",
    "        mean, std = wmean, wstd\n",
    "    else:\n",
    "        mean = data.mean(reduce_dims, skipna=True)\n",
    "        std = data.std(reduce_dims, skipna=True)\n",
    "\n",
    "    mean = mean.fillna(0.0).astype(\"float32\")\n",
    "    std = std.fillna(1.0).astype(\"float32\")\n",
    "    std = xr.where(std == 0, 1.0, std)\n",
    "    return mean, std\n",
    "\n",
    "class CoraDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for NeuralCORA-style grids with configurable temporal context.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        ds,\n",
    "        var_dict,\n",
    "        lead_time,\n",
    "        *,\n",
    "        input_steps: int = 1,\n",
    "        flatten_inputs: bool = True,\n",
    "        mean=None,\n",
    "        std=None,\n",
    "        load_into_memory=True,\n",
    "        mask: xr.DataArray | None = None,\n",
    "        weights: xr.DataArray | None = None,\n",
    "        float_dtype=\"float32\",\n",
    "    ):\n",
    "        self.ds = ds\n",
    "        self.var_dict = OrderedDict(var_dict)\n",
    "        self.lead_time = int(lead_time)\n",
    "        self.input_steps = max(1, int(input_steps))\n",
    "        self.flatten_inputs = bool(flatten_inputs)\n",
    "        self.float_dtype = float_dtype\n",
    "\n",
    "        lat_name, lon_name = _get_lat_lon_names(ds)\n",
    "        self.lat_name, self.lon_name = lat_name, lon_name\n",
    "\n",
    "        data, level_slices, level_coords = _stack_vars_levels(ds, self.var_dict, lat_name, lon_name)\n",
    "        self.level_slices = level_slices\n",
    "        self.level_coords = level_coords\n",
    "\n",
    "        if mask is not None:\n",
    "            data = data.where(mask == 0)\n",
    "\n",
    "        if mean is None or std is None:\n",
    "            mean_c, std_c = _compute_stats(data, lat_name, lon_name, mask=mask, weights=weights)\n",
    "            self.mean = mean_c if mean is None else mean\n",
    "            self.std = std_c if std is None else std\n",
    "        else:\n",
    "            self.mean, self.std = mean, std\n",
    "\n",
    "        if hasattr(self.mean, \"compute\"):\n",
    "            self.mean = self.mean.compute()\n",
    "        if hasattr(self.std, \"compute\"):\n",
    "            self.std = self.std.compute()\n",
    "\n",
    "        data = (data - self.mean) / self.std\n",
    "        data = data.astype(self.float_dtype)\n",
    "\n",
    "        if load_into_memory:\n",
    "            data = data.load()\n",
    "\n",
    "        arr = np.moveaxis(data.values.astype(self.float_dtype, copy=False), -1, 1)\n",
    "        arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        self.full_array = arr\n",
    "\n",
    "        total_steps = arr.shape[0]\n",
    "        self.output_channels = arr.shape[1]\n",
    "        min_required = (self.input_steps - 1) + self.lead_time + 1\n",
    "        if total_steps < min_required:\n",
    "            raise ValueError(\"Not enough timesteps for the requested input_steps and lead_time.\")\n",
    "\n",
    "        self.n_samples = total_steps - (self.input_steps - 1) - self.lead_time\n",
    "        if self.n_samples <= 0:\n",
    "            raise ValueError(\"Lead time and input_steps combination yields no samples.\")\n",
    "\n",
    "        windows = sliding_window_view(arr, window_shape=(self.input_steps,), axis=0)\n",
    "        inputs = windows[:self.n_samples]\n",
    "        inputs = np.moveaxis(inputs, -1, 1)  # (N, steps, C, H, W)\n",
    "        target_start = self.input_steps - 1 + self.lead_time\n",
    "        targets = arr[target_start:target_start + self.n_samples]\n",
    "\n",
    "        inputs = inputs.astype(self.float_dtype, copy=False)\n",
    "        targets = targets.astype(self.float_dtype, copy=False)\n",
    "        inputs = np.nan_to_num(inputs, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        targets = np.nan_to_num(targets, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        if self.flatten_inputs:\n",
    "            n, s, c, h, w = inputs.shape\n",
    "            inputs = inputs.reshape(n, s * c, h, w)\n",
    "            self.input_channels = s * c\n",
    "        else:\n",
    "            inputs = np.moveaxis(inputs, 2, 1)  # (N, C, steps, H, W)\n",
    "            self.input_channels = inputs.shape[1]\n",
    "\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.channels = self.output_channels  # backwards compatibility\n",
    "\n",
    "        time_coords = data[\"time\"].values\n",
    "        self.init_time = time_coords[self.input_steps - 1 : self.input_steps - 1 + self.n_samples]\n",
    "        self.valid_time = time_coords[target_start:target_start + self.n_samples]\n",
    "        self.lat = data[self.lat_name].values\n",
    "        self.lon = data[self.lon_name].values\n",
    "\n",
    "        mean_vals = np.nan_to_num(self.mean.values.astype(self.float_dtype), nan=0.0)\n",
    "        std_vals = np.nan_to_num(self.std.values.astype(self.float_dtype), nan=1.0)\n",
    "        std_vals = np.where(std_vals == 0.0, 1.0, std_vals)\n",
    "        self.mean_np = mean_vals[None, :, None, None]\n",
    "        self.std_np = std_vals[None, :, None, None]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.inputs[idx])\n",
    "        y = torch.from_numpy(self.targets[idx])\n",
    "        return x, y\n",
    "\n",
    "    def unnormalize(self, values):\n",
    "        return values * self.std_np + self.mean_np\n",
    "\n",
    "    def to_xarray(self, values, times):\n",
    "        values = np.asarray(values)  # (time, C, H, W)\n",
    "        datasets = []\n",
    "        for var, slc in self.level_slices.items():\n",
    "            field = values[:, slc, :, :]\n",
    "            if self.var_dict[var] is None:\n",
    "                datasets.append(\n",
    "                    xr.DataArray(\n",
    "                        field[:, 0],\n",
    "                        dims=(\"time\", self.lat_name, self.lon_name),\n",
    "                        coords={\"time\": times, self.lat_name: self.lat, self.lon_name: self.lon},\n",
    "                        name=var,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                level_coords = self.level_coords.get(var)\n",
    "                if level_coords is None:\n",
    "                    level_coords = np.arange(field.shape[1])\n",
    "                datasets.append(\n",
    "                    xr.DataArray(\n",
    "                        np.moveaxis(field, 1, -1),\n",
    "                        dims=(\"time\", self.lat_name, self.lon_name, \"level\"),\n",
    "                        coords={\"time\": times, self.lat_name: self.lat, self.lon_name: self.lon, \"level\": level_coords},\n",
    "                        name=var,\n",
    "                    )\n",
    "                )\n",
    "        return xr.merge(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydCQYX3XhZpr"
   },
   "outputs": [],
   "source": [
    "var_dict = OrderedDict({TARGET_VAR: None})\n",
    "batch_size = 2\n",
    "lead_time = max_lead  # number of index steps ahead (4 * TIME_STRIDE hours)\n",
    "input_steps = INPUT_STEPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYBtZGy0hZpr"
   },
   "outputs": [],
   "source": [
    "#  Flattens multiple variables (and optional pressure levels) into a single channels axis.\n",
    "#  Normalizes with train stats.\n",
    "#  Produces pairs of (x[t], y[t+lead]) in channels-first tensors for PyTorch.\n",
    "#  Exposes init_time, valid_time, etc.\n",
    "\n",
    "train_ds = CoraDataset(\n",
    "    ds_train,\n",
    "    var_dict,\n",
    "    lead_time,\n",
    "    input_steps=input_steps,\n",
    "    load_into_memory=False,\n",
    ")\n",
    "valid_ds = CoraDataset(\n",
    "    ds_valid,\n",
    "    var_dict,\n",
    "    lead_time,\n",
    "    input_steps=input_steps,\n",
    "    mean=train_ds.mean,\n",
    "    std=train_ds.std,\n",
    "    load_into_memory=False,\n",
    ")\n",
    "test_ds = CoraDataset(\n",
    "    ds_test,\n",
    "    var_dict,\n",
    "    lead_time,\n",
    "    input_steps=input_steps,\n",
    "    mean=train_ds.mean,\n",
    "    std=train_ds.std,\n",
    "    load_into_memory=False,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Chmp2XAhZpr",
    "outputId": "0df9e4d8-de4c-45df-a26f-dc2b563efa5b"
   },
   "outputs": [],
   "source": [
    "print(f'Train samples: {len(train_ds)} | Input channels: {train_ds.input_channels} | Output channels: {train_ds.output_channels}')\n",
    "print(f'Input steps: {train_ds.input_steps} | Target lead time: {train_ds.lead_time}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hWeGZpXhZpr"
   },
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekOlIgC3hZpr"
   },
   "outputs": [],
   "source": [
    "class PeriodicConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dropout=0.0):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding, padding_mode='circular')\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NeuralCoraCNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, kernel_size, out_channels, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        current = in_channels\n",
    "        for out_channels_hidden in hidden_channels:\n",
    "            layers.append(PeriodicConvBlock(current, out_channels_hidden, kernel_size, dropout))\n",
    "            current = out_channels_hidden\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "        self.head = nn.Conv2d(current, out_channels, kernel_size, padding=kernel_size // 2, padding_mode='circular')\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.head(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Fkmvr9BhZpr",
    "outputId": "d496c4ed-ad7d-4399-f452-dafae0a16bd4"
   },
   "outputs": [],
   "source": [
    "hidden_channels = [32, 128, 128, 32]\n",
    "kernel_size = 5\n",
    "model = NeuralCoraCNN(\n",
    "    train_ds.input_channels,\n",
    "    hidden_channels,\n",
    "    kernel_size,\n",
    "    out_channels=train_ds.output_channels,\n",
    ").to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of model parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJ4ccrO_hZps"
   },
   "outputs": [],
   "source": [
    "# Re-run this cell after creating the model to reset optimizer/scheduler\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJv-fWZnhZps"
   },
   "outputs": [],
   "source": [
    "# Training loop with LR tracking and scheduler support\n",
    "def fit(model, train_loader, valid_loader, optimizer, criterion, epochs, device):\n",
    "    history = {'train': [], 'valid': [], 'lr': []}\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        history['train'].append(train_loss)\n",
    "\n",
    "        val_loss = float('nan')\n",
    "        if valid_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in valid_loader:\n",
    "                    xb = xb.to(device)\n",
    "                    yb = yb.to(device)\n",
    "                    preds = model(xb)\n",
    "                    loss = criterion(preds, yb)\n",
    "                    val_loss += loss.item() * xb.size(0)\n",
    "            val_loss /= len(valid_loader.dataset)\n",
    "        history['valid'].append(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr'] if optimizer.param_groups else float('nan')\n",
    "        history['lr'].append(current_lr)\n",
    "        if 'scheduler' in globals() and scheduler is not None:\n",
    "            scheduler.step(val_loss if not np.isnan(val_loss) else train_loss)\n",
    "        msg = f\"Epoch {epoch + 1}/{epochs}: train {train_loss:.4f}\"\n",
    "        if not np.isnan(val_loss):\n",
    "            msg += f\" val {val_loss:.4f}\"\n",
    "        print(msg)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xuSm6N45hZps",
    "outputId": "0c6d6fa5-1d81-4ade-ee90-8e6641914ddc"
   },
   "outputs": [],
   "source": [
    "num_epochs = 16\n",
    "history = fit(model, train_loader, valid_loader, optimizer, criterion, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crBUw2mlhZps",
    "outputId": "87e90e1a-9729-4d4d-99ed-04217cdcab02"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history['train'], label='train')\n",
    "if not np.isnan(history['valid']).all():\n",
    "    plt.plot(history['valid'], label='valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "false",
    "id": "3F_RxkcKhZps"
   },
   "source": [
    "### Create a prediction and compute score\n",
    "\n",
    "Now that we have a model (albeit a crappy one) we can create a prediction. For this we need to create a forecast for each forecast initialization time in the testing range (2017-2018) and unnormalize it. We then convert the forecasts to a Xarray dataset which allows us to easily compute the RMSE. All of this is taken care of in the `create_predictions()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlBHXw4shZps"
   },
   "outputs": [],
   "source": [
    "def create_predictions(model, loader, dataset, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(device)\n",
    "            outputs = model(xb).cpu().numpy()\n",
    "            preds.append(outputs)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    preds = dataset.unnormalize(preds)\n",
    "    return dataset.to_xarray(preds, dataset.valid_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLGdmdFLhZps"
   },
   "outputs": [],
   "source": [
    "cnn_preds = create_predictions(model, test_loader, test_ds, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEzNMN0PhZps"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "def compute_weighted_rmse(forecast: xr.DataArray | xr.Dataset,\n",
    "                          truth: xr.DataArray,\n",
    "                          weights: xr.DataArray | None = None) -> xr.DataArray:\n",
    "    if isinstance(forecast, xr.Dataset):\n",
    "        forecast = next(iter(forecast.data_vars.values()))\n",
    "\n",
    "    if \"valid_time\" in forecast.coords:\n",
    "        obs = truth.sel(time=forecast[\"valid_time\"])\n",
    "    elif \"time\" in forecast.dims:\n",
    "        obs = truth\n",
    "    else:\n",
    "        forecast = forecast.expand_dims(time=truth.time)\n",
    "        obs = truth\n",
    "\n",
    "    sq_err = (forecast - obs) ** 2\n",
    "\n",
    "    if weights is not None:\n",
    "        norm_weights = weights / weights.sum()\n",
    "        mse = sq_err.weighted(norm_weights).mean(dim=(\"latitude\", \"longitude\"))\n",
    "    else:\n",
    "        spatial_dims = [dim for dim in sq_err.dims if dim not in {\"time\", \"lead_time\"}]\n",
    "        mse = sq_err.mean(dim=spatial_dims)\n",
    "\n",
    "    rmse_per_fc = np.sqrt(mse)\n",
    "\n",
    "    if \"time\" in rmse_per_fc.dims:\n",
    "        rmse = rmse_per_fc.mean(dim=\"time\")\n",
    "    else:\n",
    "        rmse = rmse_per_fc\n",
    "\n",
    "    rmse.name = \"RMSE\"\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0rGANDiLhZps",
    "outputId": "4f9dcb03-d7dc-42fd-fcbe-af789952565d"
   },
   "outputs": [],
   "source": [
    "truth = rename_for_score(test_ds.ds[TARGET_VAR].sel(time=test_ds.valid_time))\n",
    "preds_da = rename_for_score(cnn_preds[TARGET_VAR])\n",
    "\n",
    "# lat_weights = np.cos(np.deg2rad(truth.latitude))\n",
    "rmse = compute_weighted_rmse(preds_da, truth, weights=None)\n",
    "print(rmse.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MhhDgaJhZps"
   },
   "source": [
    "# The End\n",
    "\n",
    "This is the end of the quickstart guide. Please refer to the Jupyter notebooks in the `notebooks` directory for more examples. If you have questions, feel free to ask them as a Github Issue."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "neuralcora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
